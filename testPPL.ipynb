{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型的困惑度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from Dataset.SemEval16Task5Dataset import SemEvalXMLDataset\n",
    "from CollateFn.CollateFnBERTology import CollateFnBERTology\n",
    "\n",
    "testEnDataset = SemEvalXMLDataset(phrase=\"Test\", language=\"english\")\n",
    "testEsDataset = SemEvalXMLDataset(phrase=\"Test\", language=\"spanish\")\n",
    "testFrDataset = SemEvalXMLDataset(phrase=\"Test\", language=\"french\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "testEnDataLoader = DataLoader(testEnDataset, batch_size=16, collate_fn=CollateFnBERTology.collate_fn, shuffle=False, drop_last=False)\n",
    "testEsDataLoader = DataLoader(testEsDataset, batch_size=16, collate_fn=CollateFnBERTology.collate_fn, shuffle=False, drop_last=False)\n",
    "testFrDataLoader = DataLoader(testFrDataset, batch_size=16, collate_fn=CollateFnBERTology.collate_fn, shuffle=False, drop_last=False)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataLoader = {\n",
    "    'testEn': testEnDataLoader, # 一个句子最长有108个token\n",
    "    'testEs': testEsDataLoader, # 282\n",
    "    'testFr': testFrDataLoader # 204\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "config.is_decoder = True\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-cased\", config=config)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "prediction_logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preTrain(model, dataLoader, optimizer, scheduler=None, isTrain=False, DEVICE=\"cpu\"):\n",
    "    # 微调\n",
    "    if isTrain:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    pplList = []\n",
    "    for batchDataEncode in dataLoader:\n",
    "        batchTextEncodePlus = batchDataEncode['batchTokenizerEncode']['batchTextEncodePlus']\n",
    "        batchTextEncodePlus = batchTextEncodePlus\n",
    "        \n",
    "        nlls = []\n",
    "        stride = 1\n",
    "        for index in range(0, batchTextEncodePlus.input_ids.size(1), stride):\n",
    "            begin_loc = max(index + stride - max_length, 0)\n",
    "            end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "            trg_len = end_loc - i  # may be different from stride on last loop\n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs.loss * trg_len\n",
    "            \n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "        pplList.append(ppl)\n",
    "    \n",
    "    return sum(pplList)/len(pplList)\n",
    "\n",
    "def runBatchModel(batchDataEncode, model, DEVICE=\"cpu\"):\n",
    "\n",
    "    nlls = []\n",
    "    stride = 1\n",
    "    for index in range(0, batchDataEncode.input_ids.size(1), stride):\n",
    "        begin_loc = max(index + stride - 512, 0)\n",
    "        end_loc = min(i + stride, batchDataEncode.input_ids.size(1))\n",
    "        trg_len = end_loc - i  # may be different from stride on last loop\n",
    "        input_ids = batchDataEncode.input_ids[:, begin_loc:end_loc].to(DEVICE)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        neg_log_likelihood = outputs.loss * trg_len\n",
    "            \n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "\n",
    "    return {\n",
    "        'loss': outputs.loss,\n",
    "        'ppl': ppl\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "    model.eval()\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    sentence = \"The biggest portions but adequate\"\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    sen_len = len(tokenize_input)\n",
    "    sentence_loss = 0.\n",
    "\n",
    "    for i, word in enumerate(tokenize_input):\n",
    "        # add mask to i-th character of the sentence\n",
    "        tokenize_input[i] = '[MASK]'\n",
    "        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "\n",
    "        output = model(mask_input)\n",
    "\n",
    "        prediction_scores = output[0]\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        ps = softmax(prediction_scores[0, i]).log()\n",
    "        word_loss = ps[tensor_input[0, i]]\n",
    "        sentence_loss += word_loss.item()\n",
    "\n",
    "        tokenize_input[i] = word\n",
    "    ppl = np.exp(-sentence_loss/sen_len)\n",
    "    print(ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "config.is_decoder = True\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-cased\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def score(model, tokenizer, sentence,  mask_token_id=103):\n",
    "  tensor_input = tokenizer.encode(sentence, return_tensors='pt')\n",
    "  repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1)\n",
    "  mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]\n",
    "  masked_input = repeat_input.masked_fill(mask == 1, mask_token_id)\n",
    "  labels = repeat_input.masked_fill( masked_input != mask_token_id, -100)\n",
    "  loss,_ = model(masked_input, masked_lm_labels=labels)\n",
    "  result = np.exp(loss.item())\n",
    "  return result\n",
    "\n",
    "s = score(model, tokenizer, 't the biggest portions but adequate.')\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=dataParams.LearningRate)\n",
    "warm_up_ratio = 0.1 # 定义要预热的step\n",
    "total_steps = (len(trainDataset) // dataParams.Batchsize) * dataParams.TrainEpochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * warm_up_ratio), num_training_steps=total_steps)\n",
    "\n",
    "from torch.nn import NLLLoss\n",
    "criterion = NLLLoss()\n",
    "\n",
    "from ModelSummary.ModelOutputsRecord import ModelOutputsRecord\n",
    "\n",
    "modelOutputsRecord = ModelOutputsRecord(dataParams = dataParams, phrases=['train', 'trial', 'test'])\n",
    "\n",
    "from Model.ModelRun import ModelRun\n",
    "for epoch in range(dataParams.TrainEpochs):\n",
    "    print('*'*40 + ' '*10 + str(epoch) + ' '*10 + \"*\"*40)\n",
    "    \n",
    "    for phrase in ['train', 'trial', 'test']:\n",
    "        print(\"\\n\"+\"+\"*20+' '*20 + phrase + ' '*20 + '+'*20 + '\\n')\n",
    "        epochModelOutputs = ModelRun.runEpochModel(model, criterion, dataLoader[phrase], optimizer, scheduler, isTrain=(phrase=='train'), DEVICE=DEVICE)\n",
    "        evalResultDict = modelOutputsRecord.addEpochModelOutputs(epochModelOutputs, phrase=phrase)\n",
    "        print(modelOutputsRecord.strEvalResultDict(evalResultDict))\n",
    "    \n",
    "    bestEvalResultDict = modelOutputsRecord.analyseModel()\n",
    "    print(\"best iter is \", bestEvalResultDict['iter'])\n",
    "    print('train: ', modelOutputsRecord.strEvalResultDict(bestEvalResultDict['train']))\n",
    "    print('trial: ', modelOutputsRecord.strEvalResultDict(bestEvalResultDict['trial']))\n",
    "    print('test : ', modelOutputsRecord.strEvalResultDict(bestEvalResultDict['test' ]))\n",
    "\n",
    "modelOutputsRecord.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "def cal_ppl_bygpt2():\n",
    "    sens = [\"the biggest portions but adequate.\"]\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    inputs = tokenizer(sens, padding='max_length', max_length=50, truncation=True, return_tensors=\"pt\")\n",
    "    bs, sl = inputs['input_ids'].size()\n",
    "    outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    logits = outputs[1]\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = inputs['input_ids'][:, 1:].contiguous()\n",
    "    shift_attentions = inputs['attention_mask'][:, 1:].contiguous()\n",
    "    # Flatten the tokens\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=0, reduction=\"none\")\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).detach().reshape(bs, -1)\n",
    "    meanloss = loss.sum(1) / shift_attentions.sum(1)\n",
    "    ppl = torch.exp(meanloss).numpy().tolist()\n",
    "    return ppl\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cal_ppl_bygpt2()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
